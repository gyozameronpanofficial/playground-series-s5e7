"""
Phase 2a: é«˜æ¬¡n-gram + TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡å®Ÿè£…

4-gram/5-gramç‰¹å¾´é‡ã¨TF-IDFé‡ã¿ä»˜ã‘ã«ã‚ˆã‚‹GMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¶…è¶Š
improvement_strategy.md ã¨ GM_Differentiation_Strategy.md çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

Author: Osawa
Date: 2025-07-02
Target: CV 0.974211 â†’ 0.977211+ (æœŸå¾…åŠ¹æœ +0.003-0.005)
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import itertools
from scipy.sparse import hstack, csr_matrix
import warnings
warnings.filterwarnings('ignore')

class AdvancedNgramFeatureEngineer:
    """é«˜æ¬¡n-gram + TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢"""
    
    def __init__(self, max_ngram=5, tfidf_max_features=1000):
        self.max_ngram = max_ngram
        self.tfidf_max_features = tfidf_max_features
        self.tfidf_vectorizers = {}
        self.important_ngram_combinations = {}
        
    def create_advanced_ngram_features(self, df, target=None):
        """
        é«˜æ¬¡n-gram + TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡ã®ç”Ÿæˆ
        
        Args:
            df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
            target: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•° (å­¦ç¿’æ™‚ã®ã¿)
            
        Returns:
            æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        
        print("=== Phase 2a: é«˜æ¬¡n-gram + TF-IDFç‰¹å¾´é‡ç”Ÿæˆ ===")
        
        # 1. åŸºæœ¬å‰å‡¦ç†
        df_processed = self._preprocess_data(df)
        
        # 2. é«˜æ¬¡n-gramç‰¹å¾´é‡ç”Ÿæˆ
        df_with_ngrams = self._create_high_order_ngrams(df_processed)
        
        # 3. TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡ç”Ÿæˆ
        df_with_tfidf = self._create_tfidf_features(df_with_ngrams, target)
        
        # 4. é‡è¦ç‰¹å¾´é‡é¸æŠ (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
        df_optimized = self._optimize_features(df_with_tfidf, target)
        
        return df_optimized
    
    def _preprocess_data(self, df):
        """ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"""
        
        print("1. ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ä¸­...")
        df_processed = df.copy()
        
        # æ•°å€¤ç‰¹å¾´é‡ã®æ–‡å­—åˆ—å¤‰æ› (GMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æº–æ‹ )
        for col in df_processed.columns:
            if col not in ['id', 'Personality']:
                df_processed[col] = df_processed[col].fillna(-1).astype(str)
        
        print(f"   å¤‰æ›æ¸ˆã¿ç‰¹å¾´é‡æ•°: {len([c for c in df_processed.columns if c not in ['id', 'Personality']])}")
        
        return df_processed
    
    def _create_high_order_ngrams(self, df):
        """4-gram/5-gramé«˜æ¬¡ç‰¹å¾´é‡ç”Ÿæˆ"""
        
        print("2. é«˜æ¬¡n-gramç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        df_ngrams = df.copy()
        
        # åŸºæœ¬ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ
        base_features = [col for col in df.columns if col not in ['id', 'Personality']]
        
        # 4-gramç‰¹å¾´é‡ç”Ÿæˆ (é‡è¦ãªçµ„ã¿åˆã‚ã›ã®ã¿)
        print("   4-gramç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        important_4grams = self._get_important_4gram_combinations(base_features)
        
        for i, combo in enumerate(important_4grams):
            if len(combo) == 4:
                feature_name = f"{'_'.join(combo)}_4gram"
                df_ngrams[feature_name] = (
                    df_ngrams[combo[0]] + "_" + 
                    df_ngrams[combo[1]] + "_" + 
                    df_ngrams[combo[2]] + "_" + 
                    df_ngrams[combo[3]]
                )
        
        # 5-gramç‰¹å¾´é‡ç”Ÿæˆ (æœ€é‡è¦ãªçµ„ã¿åˆã‚ã›ã®ã¿)
        print("   5-gramç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        important_5grams = self._get_important_5gram_combinations(base_features)
        
        for i, combo in enumerate(important_5grams):
            if len(combo) == 5:
                feature_name = f"{'_'.join(combo)}_5gram"
                df_ngrams[feature_name] = (
                    df_ngrams[combo[0]] + "_" + 
                    df_ngrams[combo[1]] + "_" + 
                    df_ngrams[combo[2]] + "_" + 
                    df_ngrams[combo[3]] + "_" + 
                    df_ngrams[combo[4]]
                )
        
        ngram_count = len([c for c in df_ngrams.columns if 'gram' in c])
        print(f"   ç”Ÿæˆã•ã‚ŒãŸé«˜æ¬¡n-gramç‰¹å¾´é‡æ•°: {ngram_count}")
        
        return df_ngrams
    
    def _get_important_4gram_combinations(self, features):
        """é‡è¦ãª4-gramçµ„ã¿åˆã‚ã›ã®é¸æŠ"""
        
        # å¿ƒç†å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹4-gramçµ„ã¿åˆã‚ã›
        important_combinations = [
            # ç¤¾äº¤æ´»å‹•é–¢é€£
            ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size'],
            ['Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency'],
            
            # å¿ƒç†çŠ¶æ…‹é–¢é€£
            ['Stage_fear', 'Drained_after_socializing', 'Time_spent_Alone', 'Social_event_attendance'],
            
            # ãƒãƒ©ãƒ³ã‚¹é–¢é€£
            ['Time_spent_Alone', 'Social_event_attendance', 'Stage_fear', 'Drained_after_socializing'],
            
            # å¤–å‘æ€§æŒ‡æ¨™
            ['Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Stage_fear'],
            
            # å†…å‘æ€§æŒ‡æ¨™
            ['Time_spent_Alone', 'Stage_fear', 'Drained_after_socializing', 'Post_frequency']
        ]
        
        # æ—¢å­˜ç‰¹å¾´é‡ã¨ã®çµ„ã¿åˆã‚ã›ã®ã¿è¿”ã™
        valid_combinations = []
        for combo in important_combinations:
            if all(feature in features for feature in combo):
                valid_combinations.append(combo)
        
        return valid_combinations
    
    def _get_important_5gram_combinations(self, features):
        """é‡è¦ãª5-gramçµ„ã¿åˆã‚ã›ã®é¸æŠ (è¨ˆç®—ã‚³ã‚¹ãƒˆè€ƒæ…®ã§æœ€å°é™)"""
        
        # æœ€ã‚‚é‡è¦ãª5-gramçµ„ã¿åˆã‚ã›ã®ã¿
        important_combinations = [
            # å®Œå…¨ãªç¤¾äº¤ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency'],
            
            # å¿ƒç†ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Drained_after_socializing', 'Going_outside'],
            
            # å¤–å‘æ€§å®Œå…¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            ['Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency', 'Stage_fear']
        ]
        
        # æ—¢å­˜ç‰¹å¾´é‡ã¨ã®çµ„ã¿åˆã‚ã›ã®ã¿è¿”ã™
        valid_combinations = []
        for combo in important_combinations:
            if all(feature in features for feature in combo):
                valid_combinations.append(combo)
        
        return valid_combinations
    
    def _create_tfidf_features(self, df, target=None):
        """TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡ç”Ÿæˆ"""
        
        print("3. TF-IDFé‡ã¿ä»˜ã‘ç‰¹å¾´é‡ç”Ÿæˆä¸­...")
        df_tfidf = df.copy()
        
        # n-gramç‰¹å¾´é‡ã‚’ç‰¹å®š
        ngram_features = [col for col in df.columns if 'gram' in col]
        
        if not ngram_features:
            print("   è­¦å‘Š: n-gramç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return df_tfidf
        
        # å„n-gramç‰¹å¾´é‡ã«TF-IDFé©ç”¨
        tfidf_features_added = 0
        
        for feature in ngram_features:
            try:
                # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
                tfidf_vectorizer = TfidfVectorizer(
                    max_features=100,  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚åˆ¶é™
                    ngram_range=(1, 1),  # å˜èªãƒ¬ãƒ™ãƒ«
                    min_df=2,  # æœ€ä½å‡ºç¾å›æ•°
                    token_pattern=r'[^_]+',  # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢åŒºåˆ‡ã‚Š
                    lowercase=False
                )
                
                # æ–‡å­—åˆ—ãƒ‡ãƒ¼ã‚¿ã§TF-IDFè¨ˆç®—
                tfidf_matrix = tfidf_vectorizer.fit_transform(df[feature].astype(str))
                
                # ä¸»è¦æˆåˆ†ã®ã¿è¿½åŠ  (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
                if tfidf_matrix.shape[1] > 0:
                    # æœ€ã‚‚é‡è¦ãª5æˆåˆ†ã®ã¿è¿½åŠ 
                    top_n = min(5, tfidf_matrix.shape[1])
                    feature_names = tfidf_vectorizer.get_feature_names_out()[:top_n]
                    
                    for i, fname in enumerate(feature_names):
                        tfidf_feature_name = f"{feature}_tfidf_{fname}"
                        if tfidf_matrix.shape[1] > i:
                            df_tfidf[tfidf_feature_name] = tfidf_matrix[:, i].toarray().flatten()
                            tfidf_features_added += 1
                
                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–å™¨ã‚’ä¿å­˜
                self.tfidf_vectorizers[feature] = tfidf_vectorizer
                
            except Exception as e:
                print(f"   è­¦å‘Š: {feature}ã®TF-IDFå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        print(f"   ç”Ÿæˆã•ã‚ŒãŸTF-IDFç‰¹å¾´é‡æ•°: {tfidf_features_added}")
        
        return df_tfidf
    
    def _optimize_features(self, df, target=None):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã®ç‰¹å¾´é‡æœ€é©åŒ–"""
        
        print("4. ç‰¹å¾´é‡æœ€é©åŒ–ä¸­...")
        
        # åŸºæœ¬ç‰¹å¾´é‡ã¯ä¿æŒ
        base_columns = ['id']
        if 'Personality' in df.columns:
            base_columns.append('Personality')
        
        # å…ƒã®ç‰¹å¾´é‡ã‚‚ä¿æŒ
        original_features = ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 
                           'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']
        base_columns.extend([col for col in original_features if col in df.columns])
        
        # æ–°ã—ãç”Ÿæˆã•ã‚ŒãŸç‰¹å¾´é‡
        new_features = [col for col in df.columns if col not in base_columns]
        
        # åˆ†æ•£ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (åˆ†æ•£ãŒæ¥µã‚ã¦å°ã•ã„ç‰¹å¾´é‡ã‚’é™¤å¤–)
        if target is not None:
            print("   åˆ†æ•£ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸­...")
            filtered_features = []
            
            for feature in new_features:
                try:
                    if df[feature].dtype in ['object', 'string']:
                        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®å ´åˆã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ•°ã§åˆ¤æ–­
                        unique_ratio = df[feature].nunique() / len(df)
                        if unique_ratio > 0.01:  # 1%ä»¥ä¸Šã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ¯”ç‡
                            filtered_features.append(feature)
                    else:
                        # æ•°å€¤ç‰¹å¾´é‡ã®å ´åˆã¯åˆ†æ•£ã§åˆ¤æ–­
                        if df[feature].var() > 1e-6:
                            filtered_features.append(feature)
                except:
                    continue
            
            final_columns = base_columns + filtered_features
            print(f"   ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰: {len(new_features)} â†’ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_features)}")
        else:
            final_columns = base_columns + new_features
        
        df_optimized = df[final_columns].copy()
        
        print(f"   æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(final_columns)}")
        
        return df_optimized

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=== Phase 2a: é«˜æ¬¡n-gram + TF-IDFç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ ===")
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    try:
        train_df = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/raw/train.csv')
        test_df = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/raw/test.csv')
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°æº–å‚™
        y_train = train_df['Personality'].map({'Extrovert': 1, 'Introvert': 0})
        
    except FileNotFoundError as e:
        print(f"   ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - {e}")
        return
    
    # 2. é«˜æ¬¡n-gram + TF-IDFç‰¹å¾´é‡ç”Ÿæˆ
    print("\nğŸ”§ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    engineer = AdvancedNgramFeatureEngineer(max_ngram=5, tfidf_max_features=1000)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    train_features = engineer.create_advanced_ngram_features(train_df, target=y_train)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (åŒã˜å¤‰æ›ã‚’é©ç”¨)
    test_features = engineer.create_advanced_ngram_features(test_df, target=None)
    
    # 3. ç‰¹å¾´é‡æ•°ãƒ¬ãƒãƒ¼ãƒˆ
    original_count = len([c for c in train_df.columns if c not in ['id', 'Personality']])
    new_count = len([c for c in train_features.columns if c not in ['id', 'Personality']])
    added_count = new_count - original_count
    
    print(f"\nğŸ“Š ç‰¹å¾´é‡ç”Ÿæˆçµæœ:")
    print(f"   å…ƒã®ç‰¹å¾´é‡æ•°: {original_count}")
    print(f"   æ–°ã—ã„ç‰¹å¾´é‡æ•°: {new_count}")
    print(f"   è¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡æ•°: {added_count}")
    
    # 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    print("\nğŸ’¾ å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    train_output_path = '/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv'
    test_output_path = '/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_test_features.csv'
    
    train_features.to_csv(train_output_path, index=False)
    test_features.to_csv(test_output_path, index=False)
    
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {train_output_path}")
    print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {test_output_path}")
    
    # 5. æˆåŠŸãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nâœ… Phase 2aå®Ÿè£…å®Œäº†!")
    print(f"   æœŸå¾…CVæ”¹å–„: +0.003-0.005")
    print(f"   ç›®æ¨™CVã‚¹ã‚³ã‚¢: 0.977211+")
    print(f"   æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: CVè©•ä¾¡å®Ÿè¡Œ")
    
    return train_features, test_features

if __name__ == "__main__":
    main()