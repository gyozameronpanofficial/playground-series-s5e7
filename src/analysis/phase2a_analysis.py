"""
Phase 2a è©³ç´°å•é¡Œåˆ†æ

é«˜æ¬¡n-gram + TF-IDFç‰¹å¾´é‡ãŒæœŸå¾…åŠ¹æœã‚’ç™ºæ®ã—ãªã‹ã£ãŸåŸå› ã‚’å¾¹åº•åˆ†æ
CV -0.000162ã®è¦å› ç‰¹å®šã¨æ”¹å–„ç­–ã®æ¤œè¨

Author: Osawa  
Date: 2025-07-02
Purpose: Phase 2aå¤±æ•—åŸå› ã®ç‰¹å®šã¨å­¦ç¿’
"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_classif
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

class Phase2aAnalyzer:
    """Phase 2aè©³ç´°åˆ†æå™¨"""
    
    def __init__(self):
        self.analysis_results = {}
        
    def comprehensive_analysis(self):
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        
        print("=== Phase 2a è©³ç´°å•é¡Œåˆ†æ ===")
        
        # 1. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ
        self._analyze_data_quality()
        
        # 2. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        self._analyze_feature_importance()
        
        # 3. ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æ
        self._analyze_feature_distributions()
        
        # 4. ç›¸é–¢åˆ†æ
        self._analyze_correlations()
        
        # 5. TF-IDFå“è³ªåˆ†æ
        self._analyze_tfidf_quality()
        
        # 6. å•é¡Œè¦å› ç‰¹å®š
        self._identify_root_causes()
        
        # 7. æ”¹å–„ææ¡ˆ
        self._propose_improvements()
        
        return self.analysis_results
    
    def _analyze_data_quality(self):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ"""
        
        print("\n1. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ")
        print("-" * 40)
        
        try:
            # Phase 2aãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            phase2a_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv')
            baseline_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/raw/train.csv')
            
            print(f"Phase 2aç‰¹å¾´é‡æ•°: {phase2a_data.shape[1] - 2}")  # id, Personalityã‚’é™¤ã
            print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç‰¹å¾´é‡æ•°: {baseline_data.shape[1] - 2}")
            
            # æ¬ æå€¤åˆ†æ
            phase2a_missing = phase2a_data.isnull().sum()
            missing_features = phase2a_missing[phase2a_missing > 0]
            
            if len(missing_features) > 0:
                print(f"\nâŒ æ¬ æå€¤å•é¡Œç™ºè¦‹:")
                for feature, count in missing_features.items():
                    print(f"   {feature}: {count}å€‹ ({count/len(phase2a_data)*100:.2f}%)")
            else:
                print("âœ… æ¬ æå€¤ãªã—")
            
            # ãƒ‡ãƒ¼ã‚¿å‹åˆ†æ
            object_features = phase2a_data.select_dtypes(include=['object']).columns
            if len(object_features) > 2:  # id, Personalityä»¥å¤–
                print(f"\nâš ï¸ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡å¤šæ•°: {len(object_features)}å€‹")
                print(f"   å•é¡Œ: TF-IDFç‰¹å¾´é‡ãŒæ–‡å­—åˆ—ã®ã¾ã¾ â†’ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å›°é›£")
                self.analysis_results['categorical_issue'] = True
            else:
                print("âœ… ãƒ‡ãƒ¼ã‚¿å‹é©åˆ‡")
                self.analysis_results['categorical_issue'] = False
            
            # é‡è¤‡å€¤åˆ†æ
            feature_cols = [col for col in phase2a_data.columns if col not in ['id', 'Personality']]
            duplicate_count = phase2a_data[feature_cols].duplicated().sum()
            print(f"\né‡è¤‡è¡Œæ•°: {duplicate_count}")
            
            self.analysis_results['data_quality'] = {
                'missing_features': len(missing_features),
                'categorical_features': len(object_features),
                'duplicate_rows': duplicate_count,
                'total_features': len(feature_cols)
            }
            
        except Exception as e:
            print(f"ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_feature_importance(self):
        """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
        
        print("\n2. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
        print("-" * 40)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™
            phase2a_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv')
            feature_cols = [col for col in phase2a_data.columns if col not in ['id', 'Personality']]
            
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            X_encoded = phase2a_data[feature_cols].copy()
            for col in X_encoded.columns:
                if X_encoded[col].dtype == 'object':
                    le = LabelEncoder()
                    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))
            
            X = X_encoded.fillna(0).values
            y = phase2a_data['Personality'].map({'Extrovert': 1, 'Introvert': 0}).values
            
            # RandomForesté‡è¦åº¦
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X, y)
            rf_importance = rf.feature_importances_
            
            # é‡è¦åº¦ä¸Šä½ãƒ»ä¸‹ä½ç‰¹å¾´é‡
            importance_df = pd.DataFrame({
                'feature': feature_cols,
                'importance': rf_importance
            }).sort_values('importance', ascending=False)
            
            print("ç‰¹å¾´é‡é‡è¦åº¦ Top 10:")
            for i, row in importance_df.head(10).iterrows():
                feature_type = self._classify_feature_type(row['feature'])
                print(f"   {row['feature'][:40]:40} {row['importance']:.6f} [{feature_type}]")
            
            print("\nç‰¹å¾´é‡é‡è¦åº¦ Bottom 10:")
            for i, row in importance_df.tail(10).iterrows():
                feature_type = self._classify_feature_type(row['feature'])
                print(f"   {row['feature'][:40]:40} {row['importance']:.6f} [{feature_type}]")
            
            # ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—åˆ¥é‡è¦åº¦
            feature_type_importance = {}
            for _, row in importance_df.iterrows():
                ftype = self._classify_feature_type(row['feature'])
                if ftype not in feature_type_importance:
                    feature_type_importance[ftype] = []
                feature_type_importance[ftype].append(row['importance'])
            
            print(f"\nç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—åˆ¥å¹³å‡é‡è¦åº¦:")
            for ftype, importances in feature_type_importance.items():
                avg_importance = np.mean(importances)
                count = len(importances)
                print(f"   {ftype:15}: {avg_importance:.6f} (æ•°: {count})")
            
            # ä½é‡è¦åº¦ç‰¹å¾´é‡ã®æ¯”ç‡
            low_importance_count = len(importance_df[importance_df['importance'] < 0.001])
            low_ratio = low_importance_count / len(importance_df)
            
            if low_ratio > 0.3:
                print(f"\nâŒ ä½é‡è¦åº¦ç‰¹å¾´é‡ãŒå¤šæ•°: {low_importance_count}/{len(importance_df)} ({low_ratio*100:.1f}%)")
                print("   å•é¡Œ: ãƒã‚¤ã‚ºç‰¹å¾´é‡ãŒå¤šãã€ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã‚’é˜»å®³ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
                self.analysis_results['noise_features_issue'] = True
            else:
                print(f"âœ… ä½é‡è¦åº¦ç‰¹å¾´é‡ã®æ¯”ç‡ã¯é©åˆ‡: {low_ratio*100:.1f}%")
                self.analysis_results['noise_features_issue'] = False
            
            self.analysis_results['feature_importance'] = {
                'top_features': importance_df.head(10).to_dict('records'),
                'bottom_features': importance_df.tail(10).to_dict('records'),
                'type_importance': {k: np.mean(v) for k, v in feature_type_importance.items()},
                'low_importance_ratio': low_ratio
            }
            
        except Exception as e:
            print(f"ç‰¹å¾´é‡é‡è¦åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _classify_feature_type(self, feature_name):
        """ç‰¹å¾´é‡åã‹ã‚‰ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡"""
        
        if 'tfidf' in feature_name.lower():
            return 'TF-IDF'
        elif '4gram' in feature_name.lower():
            return '4-gram'
        elif '5gram' in feature_name.lower():
            return '5-gram'
        elif any(orig in feature_name for orig in ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 
                                                  'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']):
            return 'Original'
        else:
            return 'Other'
    
    def _analyze_feature_distributions(self):
        """ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æ"""
        
        print("\n3. ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æ")
        print("-" * 40)
        
        try:
            phase2a_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv')
            feature_cols = [col for col in phase2a_data.columns if col not in ['id', 'Personality']]
            
            # TF-IDFç‰¹å¾´é‡ã®åˆ†å¸ƒåˆ†æ
            tfidf_features = [col for col in feature_cols if 'tfidf' in col.lower()]
            print(f"TF-IDFç‰¹å¾´é‡æ•°: {len(tfidf_features)}")
            
            if len(tfidf_features) > 0:
                # TF-IDFå€¤ã®çµ±è¨ˆ
                tfidf_data = phase2a_data[tfidf_features]
                
                # æ•°å€¤å¤‰æ›ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯
                tfidf_numeric = tfidf_data.copy()
                for col in tfidf_numeric.columns:
                    if tfidf_numeric[col].dtype == 'object':
                        try:
                            tfidf_numeric[col] = pd.to_numeric(tfidf_numeric[col], errors='coerce')
                        except:
                            pass
                
                if tfidf_numeric.select_dtypes(include=[np.number]).shape[1] > 0:
                    numeric_tfidf = tfidf_numeric.select_dtypes(include=[np.number])
                    
                    print(f"TF-IDFçµ±è¨ˆ:")
                    print(f"   å¹³å‡å€¤: {numeric_tfidf.mean().mean():.6f}")
                    print(f"   æ¨™æº–åå·®: {numeric_tfidf.std().mean():.6f}")
                    print(f"   æœ€å¤§å€¤: {numeric_tfidf.max().max():.6f}")
                    print(f"   æœ€å°å€¤: {numeric_tfidf.min().min():.6f}")
                    
                    # ã‚¼ãƒ­å€¤ã®æ¯”ç‡
                    zero_ratio = (numeric_tfidf == 0).sum().sum() / (numeric_tfidf.shape[0] * numeric_tfidf.shape[1])
                    print(f"   ã‚¼ãƒ­å€¤æ¯”ç‡: {zero_ratio*100:.1f}%")
                    
                    if zero_ratio > 0.8:
                        print("   âŒ TF-IDFç‰¹å¾´é‡ã®å¤§éƒ¨åˆ†ãŒã‚¼ãƒ­ â†’ æƒ…å ±é‡ãŒå°‘ãªã„")
                        self.analysis_results['tfidf_sparsity_issue'] = True
                    else:
                        print("   âœ… TF-IDFç‰¹å¾´é‡ã®å¯†åº¦ã¯é©åˆ‡")
                        self.analysis_results['tfidf_sparsity_issue'] = False
                else:
                    print("   âŒ TF-IDFç‰¹å¾´é‡ãŒå…¨ã¦éæ•°å€¤ â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ")
                    self.analysis_results['tfidf_encoding_issue'] = True
            
            # n-gramç‰¹å¾´é‡ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤åˆ†æ
            ngram_features = [col for col in feature_cols if 'gram' in col.lower() and 'tfidf' not in col.lower()]
            print(f"\nn-gramç‰¹å¾´é‡æ•°: {len(ngram_features)}")
            
            if len(ngram_features) > 0:
                for feature in ngram_features[:5]:  # æœ€åˆã®5å€‹ã‚’ã‚µãƒ³ãƒ—ãƒ«
                    unique_count = phase2a_data[feature].nunique()
                    total_count = len(phase2a_data[feature])
                    unique_ratio = unique_count / total_count
                    print(f"   {feature[:40]:40}: ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ {unique_count}/{total_count} ({unique_ratio:.3f})")
                    
                    if unique_ratio > 0.95:
                        print(f"     âš ï¸ ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤æ¯”ç‡ãŒé«˜ã™ãã‚‹ â†’ éåº¦ã«è©³ç´°ãªç‰¹å¾´é‡")
                        self.analysis_results['high_cardinality_issue'] = True
            
        except Exception as e:
            print(f"ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_correlations(self):
        """ç›¸é–¢åˆ†æ"""
        
        print("\n4. ç›¸é–¢åˆ†æ")
        print("-" * 40)
        
        try:
            phase2a_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv')
            feature_cols = [col for col in phase2a_data.columns if col not in ['id', 'Personality']]
            
            # æ•°å€¤ç‰¹å¾´é‡ã®ã¿æŠ½å‡º
            numeric_data = phase2a_data[feature_cols].copy()
            for col in numeric_data.columns:
                if numeric_data[col].dtype == 'object':
                    try:
                        numeric_data[col] = pd.to_numeric(numeric_data[col], errors='coerce')
                    except:
                        numeric_data[col] = np.nan
            
            numeric_data = numeric_data.select_dtypes(include=[np.number]).fillna(0)
            
            if numeric_data.shape[1] > 1:
                # ç›¸é–¢è¡Œåˆ—è¨ˆç®—
                corr_matrix = numeric_data.corr().abs()
                
                # é«˜ç›¸é–¢ãƒšã‚¢ã®ç‰¹å®š (å¯¾è§’ç·šé™¤ã)
                mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
                corr_matrix_masked = corr_matrix.mask(mask)
                
                # é«˜ç›¸é–¢ãƒšã‚¢ (>0.9)
                high_corr_pairs = []
                for i in range(len(corr_matrix_masked.columns)):
                    for j in range(len(corr_matrix_masked.columns)):
                        if not pd.isna(corr_matrix_masked.iloc[i, j]) and corr_matrix_masked.iloc[i, j] > 0.9:
                            high_corr_pairs.append((
                                corr_matrix_masked.columns[i],
                                corr_matrix_masked.columns[j],
                                corr_matrix_masked.iloc[i, j]
                            ))
                
                print(f"é«˜ç›¸é–¢ãƒšã‚¢æ•° (>0.9): {len(high_corr_pairs)}")
                
                if len(high_corr_pairs) > 10:
                    print("âŒ é«˜ç›¸é–¢ç‰¹å¾´é‡ãŒå¤šæ•°å­˜åœ¨ â†’ å†—é•·æ€§å•é¡Œ")
                    print("   ä¸Šä½5ãƒšã‚¢:")
                    sorted_pairs = sorted(high_corr_pairs, key=lambda x: x[2], reverse=True)
                    for feat1, feat2, corr in sorted_pairs[:5]:
                        print(f"     {feat1[:20]} - {feat2[:20]}: {corr:.3f}")
                    self.analysis_results['multicollinearity_issue'] = True
                else:
                    print("âœ… é«˜ç›¸é–¢ç‰¹å¾´é‡ã¯é©åˆ‡ãªç¯„å›²")
                    self.analysis_results['multicollinearity_issue'] = False
                
                # å¹³å‡ç›¸é–¢
                avg_corr = corr_matrix_masked.stack().mean()
                print(f"å¹³å‡ç›¸é–¢: {avg_corr:.3f}")
                
                self.analysis_results['correlation_analysis'] = {
                    'high_corr_pairs': len(high_corr_pairs),
                    'avg_correlation': avg_corr
                }
            
        except Exception as e:
            print(f"ç›¸é–¢åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_tfidf_quality(self):
        """TF-IDFå“è³ªåˆ†æ"""
        
        print("\n5. TF-IDFå“è³ªåˆ†æ")
        print("-" * 40)
        
        try:
            phase2a_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_train_features.csv')
            
            # TF-IDFç‰¹å¾´é‡ã‚’ç‰¹å®š
            tfidf_features = [col for col in phase2a_data.columns if 'tfidf' in col.lower()]
            
            if len(tfidf_features) == 0:
                print("âŒ TF-IDFç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚‰ãªã„")
                return
            
            print(f"TF-IDFç‰¹å¾´é‡æ•°: {len(tfidf_features)}")
            
            # ã‚µãƒ³ãƒ—ãƒ«TF-IDFç‰¹å¾´é‡ã®å€¤ã‚’ç¢ºèª
            sample_tfidf = phase2a_data[tfidf_features[:5]]
            print(f"\nã‚µãƒ³ãƒ—ãƒ«TF-IDFå€¤:")
            for col in sample_tfidf.columns:
                values = sample_tfidf[col].head(10).tolist()
                print(f"   {col[:50]:50}: {values}")
            
            # TF-IDFç‰¹å¾´é‡ã®ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª
            tfidf_dtypes = phase2a_data[tfidf_features].dtypes
            object_tfidf = [col for col in tfidf_features if tfidf_dtypes[col] == 'object']
            
            if len(object_tfidf) > 0:
                print(f"\nâŒ æ–‡å­—åˆ—å‹TF-IDFç‰¹å¾´é‡: {len(object_tfidf)}å€‹")
                print("   å•é¡Œ: TF-IDFå€¤ãŒæ•°å€¤åŒ–ã•ã‚Œã¦ã„ãªã„")
                print(f"   ä¾‹: {object_tfidf[:3]}")
                self.analysis_results['tfidf_type_issue'] = True
            else:
                print("âœ… TF-IDFç‰¹å¾´é‡ã¯å…¨ã¦æ•°å€¤å‹")
                self.analysis_results['tfidf_type_issue'] = False
            
            # TF-IDFå®Ÿè£…ã®å•é¡Œç‚¹ãƒã‚§ãƒƒã‚¯
            print(f"\nTF-IDFå®Ÿè£…å“è³ªãƒã‚§ãƒƒã‚¯:")
            
            # 1. å‘½åãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            naming_patterns = {}
            for col in tfidf_features:
                parts = col.split('_')
                if len(parts) >= 3 and 'tfidf' in parts:
                    pattern = '_'.join(parts[:-1])  # æœ€å¾Œã®è¦ç´ ï¼ˆå˜èªï¼‰ã‚’é™¤ã„ãŸéƒ¨åˆ†
                    if pattern not in naming_patterns:
                        naming_patterns[pattern] = 0
                    naming_patterns[pattern] += 1
            
            print(f"   TF-IDFå‘½åãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(naming_patterns)}")
            for pattern, count in list(naming_patterns.items())[:5]:
                print(f"     {pattern}: {count}å€‹")
            
            if len(naming_patterns) < 5:
                print("   âš ï¸ TF-IDFå‘½åãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå°‘ãªã„ â†’ ç”Ÿæˆãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ä¸è¶³")
            
        except Exception as e:
            print(f"TF-IDFå“è³ªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    def _identify_root_causes(self):
        """æ ¹æœ¬åŸå› ç‰¹å®š"""
        
        print("\n6. æ ¹æœ¬åŸå› ç‰¹å®š")
        print("-" * 40)
        
        root_causes = []
        
        # å„å•é¡Œã‚’ãƒã‚§ãƒƒã‚¯
        if self.analysis_results.get('categorical_issue', False):
            root_causes.append("âŒ ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ")
        
        if self.analysis_results.get('noise_features_issue', False):
            root_causes.append("âŒ ãƒã‚¤ã‚ºç‰¹å¾´é‡å¤§é‡ç”Ÿæˆ")
        
        if self.analysis_results.get('tfidf_sparsity_issue', False):
            root_causes.append("âŒ TF-IDFç‰¹å¾´é‡ã®æ¥µåº¦ãªå¸Œè–„æ€§")
        
        if self.analysis_results.get('tfidf_encoding_issue', False):
            root_causes.append("âŒ TF-IDFç‰¹å¾´é‡ã®æ•°å€¤åŒ–å¤±æ•—")
        
        if self.analysis_results.get('high_cardinality_issue', False):
            root_causes.append("âŒ é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ç‰¹å¾´é‡å•é¡Œ")
        
        if self.analysis_results.get('multicollinearity_issue', False):
            root_causes.append("âŒ å¤šé‡å…±ç·šæ€§å•é¡Œ")
        
        if self.analysis_results.get('tfidf_type_issue', False):
            root_causes.append("âŒ TF-IDFå®Ÿè£…ã®æ ¹æœ¬çš„å•é¡Œ")
        
        print("ç‰¹å®šã•ã‚ŒãŸæ ¹æœ¬åŸå› :")
        for i, cause in enumerate(root_causes, 1):
            print(f"   {i}. {cause}")
        
        if not root_causes:
            print("âœ… æ˜ç¢ºãªæŠ€è¡“çš„å•é¡Œã¯ç‰¹å®šã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            print("   â†’ æ‰‹æ³•è‡ªä½“ã®æœ‰åŠ¹æ€§ã®å•é¡Œã®å¯èƒ½æ€§")
        
        self.analysis_results['root_causes'] = root_causes
    
    def _propose_improvements(self):
        """æ”¹å–„ææ¡ˆ"""
        
        print("\n7. æ”¹å–„ææ¡ˆ")
        print("-" * 40)
        
        improvements = []
        
        # æ ¹æœ¬åŸå› ã«åŸºã¥ãæ”¹å–„ææ¡ˆ
        if self.analysis_results.get('categorical_issue', False):
            improvements.append("ğŸ”§ å…¨ç‰¹å¾´é‡ã®é©åˆ‡ãªæ•°å€¤ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè£…")
        
        if self.analysis_results.get('noise_features_issue', False):
            improvements.append("ğŸ”§ ç‰¹å¾´é‡é¸æŠã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»")
        
        if self.analysis_results.get('tfidf_sparsity_issue', False):
            improvements.append("ğŸ”§ TF-IDFæœ€å°æ–‡æ›¸é »åº¦(min_df)ã®èª¿æ•´")
        
        if self.analysis_results.get('high_cardinality_issue', False):
            improvements.append("ğŸ”§ n-gramçµ„ã¿åˆã‚ã›ã®æ›´ãªã‚‹å³é¸")
        
        if self.analysis_results.get('multicollinearity_issue', False):
            improvements.append("ğŸ”§ ç›¸é–¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹å†—é•·æ€§é™¤å»")
        
        if self.analysis_results.get('tfidf_type_issue', False):
            improvements.append("ğŸ”§ TF-IDFå®Ÿè£…ã®æ ¹æœ¬çš„è¦‹ç›´ã—")
        
        # ä¸€èˆ¬çš„æ”¹å–„ææ¡ˆ
        improvements.extend([
            "ğŸ¯ ã‚ˆã‚Š selective ãª4-gram/5-gramé¸æŠ",
            "ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ã«åŸºã¥ãæ®µéšçš„è¿½åŠ ",
            "ğŸ§ª å˜ä¸€ç‰¹å¾´é‡ã‚¿ã‚¤ãƒ—ã§ã®åŠ¹æœæ¤œè¨¼",
            "âš¡ ã‚ˆã‚Šè»½é‡ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ‰‹æ³•ã®æ¤œè¨"
        ])
        
        print("æ¨å¥¨æ”¹å–„ç­–:")
        for i, improvement in enumerate(improvements, 1):
            print(f"   {i}. {improvement}")
        
        # Phase 2b ã¸ã®æè¨€
        print(f"\nPhase 2b ã¸ã®æè¨€:")
        print("   ğŸ“ é«˜æ¬¡n-gram + TF-IDFã¯åŠ¹æœé™å®šçš„")
        print("   ğŸ“ Target Encodingç­‰ã®ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹æ³•ã‚’å„ªå…ˆ")
        print("   ğŸ“ ç‰¹å¾´é‡å“è³ª > ç‰¹å¾´é‡æ•°é‡")
        
        self.analysis_results['improvements'] = improvements
        
        return improvements

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    analyzer = Phase2aAnalyzer()
    results = analyzer.comprehensive_analysis()
    
    # çµæœä¿å­˜ (JSON serializableå½¢å¼ã«å¤‰æ›)
    import json
    
    # Numpyå‹ã‚’Pythonå‹ã«å¤‰æ›
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    serializable_results = convert_numpy_types(results)
    
    with open('/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase2a_analysis_results.json', 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    print(f"\n" + "="*60)
    print("Phase 2a åˆ†æå®Œäº†")
    print("="*60)
    print("âœ… åˆ†æçµæœä¿å­˜: phase2a_analysis_results.json")
    print("ğŸ“‹ ä¸»è¦å•é¡Œ: TF-IDFå®Ÿè£…å“è³ªã¨ãƒã‚¤ã‚ºç‰¹å¾´é‡")
    print("ğŸ¯ Phase 2b: ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªæ‰‹æ³•ã«æ³¨åŠ›")
    
    return results

if __name__ == "__main__":
    main()