"""
æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«
ã“ã‚Œã¾ã§ã®åˆ†æçµæœã‚’çµ±åˆã—ãŸå®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import TargetEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb

class FinalSubmissionModel:
    """æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.config = {
            'RANDOM_STATE': 42,
            'N_SPLITS': 5,
            'TARGET_COL': 'Personality',
            'TARGET_MAPPING': {"Extrovert": 0, "Introvert": 1},
            'DATA_PATH': Path('/Users/osawa/kaggle/playground-series-s5e7/data/raw')
        }
        
    def load_and_preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        print("=== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç† ===")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df = pd.read_csv(self.config['DATA_PATH'] / 'train.csv')
        test_df = pd.read_csv(self.config['DATA_PATH'] / 'test.csv')
        
        # ç‰¹å¾´é‡æŠ½å‡º
        feature_cols = [col for col in train_df.columns 
                       if col not in ['id', self.config['TARGET_COL']]]
        
        X_train = train_df[feature_cols].copy()
        y_train = train_df[self.config['TARGET_COL']].map(self.config['TARGET_MAPPING'])
        X_test = test_df[feature_cols].copy()
        
        print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
        # GMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰å‡¦ç†
        # æ•°å€¤ç‰¹å¾´é‡å‡¦ç†
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
        X_train[numeric_cols] = X_train[numeric_cols].fillna(-1).astype(np.int16).astype(str)
        X_test[numeric_cols] = X_test[numeric_cols].fillna(-1).astype(np.int16).astype(str)
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡å‡¦ç†
        categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
        X_train[categorical_cols] = X_train[categorical_cols].astype(str).fillna("missing")
        X_test[categorical_cols] = X_test[categorical_cols].astype(str).fillna("missing")
        
        print(f"å‰å‡¦ç†å¾Œå½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
        return X_train, y_train, X_test
    
    def create_advanced_features(self, X_train, X_test):
        """é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ"""
        print("\n=== é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ ===")
        
        from itertools import combinations
        
        original_cols = list(X_train.columns)
        print(f"å…ƒã®ç‰¹å¾´é‡æ•°: {len(original_cols)}")
        
        # 2-gramç‰¹å¾´é‡
        print("2-gramç‰¹å¾´é‡ç”Ÿæˆ...")
        for col1, col2 in combinations(original_cols, 2):
            feature_name = f"{col1}-{col2}"
            X_train[feature_name] = X_train[col1] + "-" + X_train[col2]
            X_test[feature_name] = X_test[col1] + "-" + X_test[col2]
        
        print(f"2-gramå¾Œ: {X_train.shape[1]} ç‰¹å¾´é‡")
        
        # 3-gramç‰¹å¾´é‡
        print("3-gramç‰¹å¾´é‡ç”Ÿæˆ...")
        for col1, col2, col3 in combinations(original_cols, 3):
            feature_name = f"{col1}-{col2}-{col3}"
            X_train[feature_name] = (X_train[col1] + "-" + 
                                    X_train[col2] + "-" + 
                                    X_train[col3])
            X_test[feature_name] = (X_test[col1] + "-" + 
                                   X_test[col2] + "-" + 
                                   X_test[col3])
        
        print(f"3-gramå¾Œ: {X_train.shape[1]} ç‰¹å¾´é‡")
        
        # 4-gramç‰¹å¾´é‡ï¼ˆé¸æŠçš„ã«è¿½åŠ ï¼‰
        print("4-gramç‰¹å¾´é‡ç”Ÿæˆï¼ˆé‡è¦ãªçµ„ã¿åˆã‚ã›ã®ã¿ï¼‰...")
        important_4grams = 0
        for col1, col2, col3, col4 in combinations(original_cols, 4):
            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã®ãŸã‚æœ€åˆã®10å€‹ã®ã¿
            if important_4grams >= 10:
                break
            feature_name = f"{col1}-{col2}-{col3}-{col4}"
            X_train[feature_name] = (X_train[col1] + "-" + 
                                    X_train[col2] + "-" + 
                                    X_train[col3] + "-" + 
                                    X_train[col4])
            X_test[feature_name] = (X_test[col1] + "-" + 
                                   X_test[col2] + "-" + 
                                   X_test[col3] + "-" + 
                                   X_test[col4])
            important_4grams += 1
        
        print(f"æœ€çµ‚ç‰¹å¾´é‡æ•°: {X_train.shape[1]} (4-gram {important_4grams}å€‹è¿½åŠ )")
        
        return X_train, X_test
    
    def setup_models(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç¾¤è¨­å®š"""
        print("\n=== æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç¾¤è¨­å®š ===")
        
        models = {
            # é«˜æ€§èƒ½XGBoostï¼ˆåˆ†æçµæœã‹ã‚‰æœ€é©åŒ–ï¼‰
            'XGBoost_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', xgb.XGBClassifier(
                    objective='binary:logistic',
                    eval_metric='logloss',
                    learning_rate=0.01,  # ã‚ˆã‚Šæ…é‡ãªå­¦ç¿’ç‡
                    n_estimators=2000,   # ååˆ†ãªã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                    max_depth=6,
                    colsample_bytree=0.7,
                    subsample=0.8,
                    reg_alpha=0.3,
                    reg_lambda=1.0,
                    min_child_weight=4,
                    random_state=self.config['RANDOM_STATE'],
                    verbosity=0
                ))
            ]),
            
            # é«˜æ€§èƒ½LightGBM
            'LightGBM_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', lgb.LGBMClassifier(
                    objective='binary',
                    metric='logloss',
                    learning_rate=0.01,
                    n_estimators=2000,
                    max_depth=6,
                    colsample_bytree=0.7,
                    subsample=0.8,
                    reg_alpha=0.3,
                    reg_lambda=1.0,
                    min_child_samples=10,
                    num_leaves=50,
                    random_state=self.config['RANDOM_STATE'],
                    verbosity=-1
                ))
            ]),
            
            # é«˜æ€§èƒ½CatBoost
            'CatBoost_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', cb.CatBoostClassifier(
                    loss_function='Logloss',
                    learning_rate=0.01,
                    iterations=2000,
                    max_depth=6,
                    reg_lambda=1.0,
                    subsample=0.8,
                    random_strength=0.5,
                    bagging_temperature=0.5,
                    random_state=self.config['RANDOM_STATE'],
                    verbose=False
                ))
            ]),
            
            # æœ€é©åŒ–RandomForest
            'RandomForest_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', RandomForestClassifier(
                    n_estimators=500,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    max_features='sqrt',
                    max_samples=0.8,
                    random_state=self.config['RANDOM_STATE']
                ))
            ])
        }
        
        print(f"è¨­å®šå®Œäº†: {len(models)} ãƒ¢ãƒ‡ãƒ«")
        return models
    
    def train_models_with_oof(self, models, X_train, y_train):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨OOFäºˆæ¸¬ç”Ÿæˆ"""
        print("\n=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»OOFäºˆæ¸¬ç”Ÿæˆ ===")
        
        cv = StratifiedKFold(
            n_splits=self.config['N_SPLITS'],
            shuffle=True,
            random_state=self.config['RANDOM_STATE']
        )
        
        oof_predictions = {}
        model_scores = {}
        
        for name, model in models.items():
            print(f"\n--- {name} è¨“ç·´ä¸­ ---")
            
            oof_preds = np.zeros(len(X_train))
            fold_scores = []
            
            for fold, (train_idx, valid_idx) in enumerate(cv.split(X_train, y_train)):
                X_fold_train = X_train.iloc[train_idx]
                X_fold_valid = X_train.iloc[valid_idx]
                y_fold_train = y_train.iloc[train_idx]
                y_fold_valid = y_train.iloc[valid_idx]
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model.fit(X_fold_train, y_fold_train)
                
                # OOFäºˆæ¸¬
                valid_preds = model.predict_proba(X_fold_valid)[:, 1]
                oof_preds[valid_idx] = valid_preds
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚³ã‚¢
                fold_score = accuracy_score(y_fold_valid, (valid_preds >= 0.5).astype(int))
                fold_scores.append(fold_score)
                print(f"  Fold {fold+1}: {fold_score:.6f}")
            
            # å…¨ä½“ã‚¹ã‚³ã‚¢
            overall_score = accuracy_score(y_train, (oof_preds >= 0.5).astype(int))
            model_scores[name] = {
                'score': overall_score,
                'std': np.std(fold_scores)
            }
            oof_predictions[name] = oof_preds
            
            print(f"  {name} å…¨ä½“ã‚¹ã‚³ã‚¢: {overall_score:.6f} (Â±{np.std(fold_scores):.6f})")
        
        return oof_predictions, model_scores
    
    def create_optimized_ensemble(self, oof_predictions, y_train):
        """æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
        print(f"\n=== æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ===")
        
        # OOFäºˆæ¸¬ã‚’DataFrameã«å¤‰æ›
        oof_df = pd.DataFrame(oof_predictions)
        
        # è¤‡æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã‚’è©¦è¡Œ
        ensemble_results = {}
        
        # 1. å˜ç´”å¹³å‡
        simple_avg = oof_df.mean(axis=1)
        simple_score = accuracy_score(y_train, (simple_avg >= 0.5).astype(int))
        ensemble_results['simple_average'] = simple_score
        print(f"å˜ç´”å¹³å‡: {simple_score:.6f}")
        
        # 2. æ€§èƒ½ãƒ™ãƒ¼ã‚¹é‡ã¿ä»˜ãå¹³å‡
        weights = []
        for col in oof_df.columns:
            pred = oof_df[col]
            score = accuracy_score(y_train, (pred >= 0.5).astype(int))
            weights.append(score)
        
        weights = np.array(weights)
        weights = weights / weights.sum()  # æ­£è¦åŒ–
        
        weighted_avg = np.average(oof_df.values, axis=1, weights=weights)
        weighted_score = accuracy_score(y_train, (weighted_avg >= 0.5).astype(int))
        ensemble_results['weighted_average'] = weighted_score
        print(f"é‡ã¿ä»˜ãå¹³å‡: {weighted_score:.6f}")
        print(f"é‡ã¿: {dict(zip(oof_df.columns, weights))}")
        
        # 3. LogisticRegression ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
        meta_model = LogisticRegression(
            C=0.1, 
            max_iter=1000, 
            random_state=self.config['RANDOM_STATE']
        )
        meta_model.fit(oof_df, y_train)
        meta_preds = meta_model.predict_proba(oof_df)[:, 1]
        meta_score = accuracy_score(y_train, (meta_preds >= 0.5).astype(int))
        ensemble_results['meta_model'] = meta_score
        print(f"ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«: {meta_score:.6f}")
        
        # æœ€è‰¯æ‰‹æ³•ã‚’é¸æŠ
        best_method = max(ensemble_results.items(), key=lambda x: x[1])
        print(f"\næœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {best_method[0]} ({best_method[1]:.6f})")\n        \n        return best_method, weights, meta_model\n    \n    def generate_test_predictions(self, models, X_train, y_train, X_test, \n                                 best_method, weights, meta_model):\n        \"\"\"ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ\"\"\"\n        print(f\"\\n=== ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ ===\")\n        \n        # å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´\n        test_predictions = {}\n        for name, model in models.items():\n            print(f\"{name} å…¨ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´ä¸­...\")\n            model.fit(X_train, y_train)\n            test_preds = model.predict_proba(X_test)[:, 1]\n            test_predictions[name] = test_preds\n        \n        # æœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’çµ±åˆ\n        test_df = pd.DataFrame(test_predictions)\n        \n        if best_method[0] == 'simple_average':\n            final_preds = test_df.mean(axis=1)\n        elif best_method[0] == 'weighted_average':\n            final_preds = np.average(test_df.values, axis=1, weights=weights)\n        else:  # meta_model\n            final_preds = meta_model.predict_proba(test_df)[:, 1]\n        \n        return final_preds\n    \n    def create_submission_file(self, test_predictions, test_df_original):\n        \"\"\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\"\"\"\n        print(f\"\\n=== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===\")\n        \n        # äºŒå€¤åˆ†é¡ã«å¤‰æ›\n        binary_preds = (test_predictions >= 0.5).astype(int)\n        \n        # ãƒ©ãƒ™ãƒ«ã‚’æ–‡å­—åˆ—ã«æˆ»ã™\n        reverse_mapping = {v: k for k, v in self.config['TARGET_MAPPING'].items()}\n        string_preds = [reverse_mapping[pred] for pred in binary_preds]\n        \n        # æå‡ºDataFrameä½œæˆ\n        submission = pd.DataFrame({\n            'id': test_df_original['id'],\n            self.config['TARGET_COL']: string_preds\n        })\n        \n        # ä¿å­˜\n        submission_path = '/Users/osawa/kaggle/playground-series-s5e7/submissions/final_submission.csv'\n        submission.to_csv(submission_path, index=False)\n        \n        print(f\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {submission_path}\")\n        print(f\"æå‡ºãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {submission.shape}\")\n        print(f\"äºˆæ¸¬åˆ†å¸ƒ:\")\n        print(submission[self.config['TARGET_COL']].value_counts())\n        \n        return submission\n    \n    def run_final_submission(self):\n        \"\"\"æœ€çµ‚æå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\"\"\"\n        print(\"=== æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ ===\\n\")\n        \n        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†\n        X_train, y_train, X_test = self.load_and_preprocess_data()\n        test_df_original = pd.read_csv(self.config['DATA_PATH'] / 'test.csv')\n        \n        # 2. é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ\n        X_train_enhanced, X_test_enhanced = self.create_advanced_features(X_train, X_test)\n        \n        # 3. æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š\n        models = self.setup_models()\n        \n        # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»OOFäºˆæ¸¬\n        oof_predictions, model_scores = self.train_models_with_oof(\n            models, X_train_enhanced, y_train\n        )\n        \n        # 5. æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n        best_method, weights, meta_model = self.create_optimized_ensemble(\n            oof_predictions, y_train\n        )\n        \n        # 6. ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ\n        test_predictions = self.generate_test_predictions(\n            models, X_train_enhanced, y_train, X_test_enhanced,\n            best_method, weights, meta_model\n        )\n        \n        # 7. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n        submission = self.create_submission_file(test_predictions, test_df_original)\n        \n        # 8. çµæœã‚µãƒãƒªãƒ¼\n        print(f\"\\n=== æœ€çµ‚çµæœ ===\")\n        print(f\"æœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«: {max(model_scores.items(), key=lambda x: x[1]['score'])}\")\n        print(f\"æœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {best_method[0]} ({best_method[1]:.6f})\")\n        print(f\"ç›®æ¨™0.975708ã¨ã®å·®: {0.975708 - best_method[1]:+.6f}\")\n        \n        if best_method[1] >= 0.975708:\n            print(\"ğŸ‰ ç›®æ¨™ã‚¹ã‚³ã‚¢é”æˆï¼\")\n        else:\n            gap = 0.975708 - best_method[1]\n            print(f\"ç›®æ¨™ã¾ã§{gap:.6f}ã®æ”¹å–„ãŒå¿…è¦\")\n        \n        return {\n            'final_score': best_method[1],\n            'model_scores': model_scores,\n            'submission': submission\n        }\n\nif __name__ == \"__main__\":\n    final_model = FinalSubmissionModel()\n    results = final_model.run_final_submission()"