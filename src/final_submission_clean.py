"""
æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«ï¼ˆã‚¯ãƒªãƒ¼ãƒ³ç‰ˆï¼‰
ã“ã‚Œã¾ã§ã®åˆ†æçµæœã‚’çµ±åˆã—ãŸå®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
"""

import pandas as pd
import numpy as np
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import TargetEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb

class FinalSubmissionModel:
    """æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.config = {
            'RANDOM_STATE': 42,
            'N_SPLITS': 5,
            'TARGET_COL': 'Personality',
            'TARGET_MAPPING': {"Extrovert": 0, "Introvert": 1},
            'DATA_PATH': Path('/Users/osawa/kaggle/playground-series-s5e7/data/raw')
        }
        
    def load_and_preprocess_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†"""
        print("=== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç† ===")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df = pd.read_csv(self.config['DATA_PATH'] / 'train.csv')
        test_df = pd.read_csv(self.config['DATA_PATH'] / 'test.csv')
        
        # ç‰¹å¾´é‡æŠ½å‡º
        feature_cols = [col for col in train_df.columns 
                       if col not in ['id', self.config['TARGET_COL']]]
        
        X_train = train_df[feature_cols].copy()
        y_train = train_df[self.config['TARGET_COL']].map(self.config['TARGET_MAPPING'])
        X_test = test_df[feature_cols].copy()
        
        print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
        # GMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰å‡¦ç†
        # æ•°å€¤ç‰¹å¾´é‡å‡¦ç†
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()
        X_train[numeric_cols] = X_train[numeric_cols].fillna(-1).astype(np.int16).astype(str)
        X_test[numeric_cols] = X_test[numeric_cols].fillna(-1).astype(np.int16).astype(str)
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡å‡¦ç†
        categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()
        X_train[categorical_cols] = X_train[categorical_cols].astype(str).fillna("missing")
        X_test[categorical_cols] = X_test[categorical_cols].astype(str).fillna("missing")
        
        print(f"å‰å‡¦ç†å¾Œå½¢çŠ¶: X_train={X_train.shape}, X_test={X_test.shape}")
        
        return X_train, y_train, X_test
    
    def create_advanced_features(self, X_train, X_test):
        """é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ"""
        print("=== é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ ===")
        
        from itertools import combinations
        
        original_cols = list(X_train.columns)
        print(f"å…ƒã®ç‰¹å¾´é‡æ•°: {len(original_cols)}")
        
        # 2-gramç‰¹å¾´é‡
        print("2-gramç‰¹å¾´é‡ç”Ÿæˆ...")
        for col1, col2 in combinations(original_cols, 2):
            feature_name = f"{col1}-{col2}"
            X_train[feature_name] = X_train[col1] + "-" + X_train[col2]
            X_test[feature_name] = X_test[col1] + "-" + X_test[col2]
        
        print(f"2-gramå¾Œ: {X_train.shape[1]} ç‰¹å¾´é‡")
        
        # 3-gramç‰¹å¾´é‡
        print("3-gramç‰¹å¾´é‡ç”Ÿæˆ...")
        for col1, col2, col3 in combinations(original_cols, 3):
            feature_name = f"{col1}-{col2}-{col3}"
            X_train[feature_name] = (X_train[col1] + "-" + 
                                    X_train[col2] + "-" + 
                                    X_train[col3])
            X_test[feature_name] = (X_test[col1] + "-" + 
                                   X_test[col2] + "-" + 
                                   X_test[col3])
        
        print(f"æœ€çµ‚ç‰¹å¾´é‡æ•°: {X_train.shape[1]}")
        
        return X_train, X_test
    
    def setup_models(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ç¾¤è¨­å®š"""
        print("=== æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ç¾¤è¨­å®š ===")
        
        models = {
            # é«˜æ€§èƒ½XGBoost
            'XGBoost_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', xgb.XGBClassifier(
                    objective='binary:logistic',
                    eval_metric='logloss',
                    learning_rate=0.01,
                    n_estimators=2000,
                    max_depth=6,
                    colsample_bytree=0.7,
                    subsample=0.8,
                    reg_alpha=0.3,
                    reg_lambda=1.0,
                    min_child_weight=4,
                    random_state=self.config['RANDOM_STATE'],
                    verbosity=0
                ))
            ]),
            
            # é«˜æ€§èƒ½LightGBM
            'LightGBM_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', lgb.LGBMClassifier(
                    objective='binary',
                    metric='logloss',
                    learning_rate=0.01,
                    n_estimators=2000,
                    max_depth=6,
                    colsample_bytree=0.7,
                    subsample=0.8,
                    reg_alpha=0.3,
                    reg_lambda=1.0,
                    min_child_samples=10,
                    num_leaves=50,
                    random_state=self.config['RANDOM_STATE'],
                    verbosity=-1
                ))
            ]),
            
            # é«˜æ€§èƒ½CatBoost
            'CatBoost_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', cb.CatBoostClassifier(
                    loss_function='Logloss',
                    learning_rate=0.01,
                    iterations=2000,
                    max_depth=6,
                    reg_lambda=1.0,
                    subsample=0.8,
                    random_strength=0.5,
                    bagging_temperature=0.5,
                    random_state=self.config['RANDOM_STATE'],
                    verbose=False
                ))
            ]),
            
            # æœ€é©åŒ–RandomForest
            'RandomForest_Optimized': Pipeline([
                ('encoder', TargetEncoder(random_state=self.config['RANDOM_STATE'])),
                ('model', RandomForestClassifier(
                    n_estimators=500,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    max_features='sqrt',
                    max_samples=0.8,
                    random_state=self.config['RANDOM_STATE']
                ))
            ])
        }
        
        print(f"è¨­å®šå®Œäº†: {len(models)} ãƒ¢ãƒ‡ãƒ«")
        return models
    
    def train_models_with_oof(self, models, X_train, y_train):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨OOFäºˆæ¸¬ç”Ÿæˆ"""
        print("=== ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»OOFäºˆæ¸¬ç”Ÿæˆ ===")
        
        cv = StratifiedKFold(
            n_splits=self.config['N_SPLITS'],
            shuffle=True,
            random_state=self.config['RANDOM_STATE']
        )
        
        oof_predictions = {}
        model_scores = {}
        
        for name, model in models.items():
            print(f"\n--- {name} è¨“ç·´ä¸­ ---")
            
            oof_preds = np.zeros(len(X_train))
            fold_scores = []
            
            for fold, (train_idx, valid_idx) in enumerate(cv.split(X_train, y_train)):
                X_fold_train = X_train.iloc[train_idx]
                X_fold_valid = X_train.iloc[valid_idx]
                y_fold_train = y_train.iloc[train_idx]
                y_fold_valid = y_train.iloc[valid_idx]
                
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                model.fit(X_fold_train, y_fold_train)
                
                # OOFäºˆæ¸¬
                valid_preds = model.predict_proba(X_fold_valid)[:, 1]
                oof_preds[valid_idx] = valid_preds
                
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚³ã‚¢
                fold_score = accuracy_score(y_fold_valid, (valid_preds >= 0.5).astype(int))
                fold_scores.append(fold_score)
                print(f"  Fold {fold+1}: {fold_score:.6f}")
            
            # å…¨ä½“ã‚¹ã‚³ã‚¢
            overall_score = accuracy_score(y_train, (oof_preds >= 0.5).astype(int))
            model_scores[name] = {
                'score': overall_score,
                'std': np.std(fold_scores)
            }
            oof_predictions[name] = oof_preds
            
            print(f"  {name} å…¨ä½“ã‚¹ã‚³ã‚¢: {overall_score:.6f} (Â±{np.std(fold_scores):.6f})")
        
        return oof_predictions, model_scores
    
    def create_optimized_ensemble(self, oof_predictions, y_train):
        """æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"""
        print("=== æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« ===")
        
        # OOFäºˆæ¸¬ã‚’DataFrameã«å¤‰æ›
        oof_df = pd.DataFrame(oof_predictions)
        
        # è¤‡æ•°ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã‚’è©¦è¡Œ
        ensemble_results = {}
        
        # 1. å˜ç´”å¹³å‡
        simple_avg = oof_df.mean(axis=1)
        simple_score = accuracy_score(y_train, (simple_avg >= 0.5).astype(int))
        ensemble_results['simple_average'] = simple_score
        print(f"å˜ç´”å¹³å‡: {simple_score:.6f}")
        
        # 2. æ€§èƒ½ãƒ™ãƒ¼ã‚¹é‡ã¿ä»˜ãå¹³å‡
        weights = []
        for col in oof_df.columns:
            pred = oof_df[col]
            score = accuracy_score(y_train, (pred >= 0.5).astype(int))
            weights.append(score)
        
        weights = np.array(weights)
        weights = weights / weights.sum()  # æ­£è¦åŒ–
        
        weighted_avg = np.average(oof_df.values, axis=1, weights=weights)
        weighted_score = accuracy_score(y_train, (weighted_avg >= 0.5).astype(int))
        ensemble_results['weighted_average'] = weighted_score
        print(f"é‡ã¿ä»˜ãå¹³å‡: {weighted_score:.6f}")
        print(f"é‡ã¿: {dict(zip(oof_df.columns, weights))}")
        
        # 3. LogisticRegression ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«
        meta_model = LogisticRegression(
            C=0.1, 
            max_iter=1000, 
            random_state=self.config['RANDOM_STATE']
        )
        meta_model.fit(oof_df, y_train)
        meta_preds = meta_model.predict_proba(oof_df)[:, 1]
        meta_score = accuracy_score(y_train, (meta_preds >= 0.5).astype(int))
        ensemble_results['meta_model'] = meta_score
        print(f"ãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«: {meta_score:.6f}")
        
        # æœ€è‰¯æ‰‹æ³•ã‚’é¸æŠ
        best_method = max(ensemble_results.items(), key=lambda x: x[1])
        print(f"\næœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {best_method[0]} ({best_method[1]:.6f})")
        
        return best_method, weights, meta_model
    
    def generate_test_predictions(self, models, X_train, y_train, X_test, 
                                 best_method, weights, meta_model):
        """ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ"""
        print("=== ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ ===")
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´
        test_predictions = {}
        for name, model in models.items():
            print(f"{name} å…¨ãƒ‡ãƒ¼ã‚¿ã§å†è¨“ç·´ä¸­...")
            model.fit(X_train, y_train)
            test_preds = model.predict_proba(X_test)[:, 1]
            test_predictions[name] = test_preds
        
        # æœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã§ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’çµ±åˆ
        test_df = pd.DataFrame(test_predictions)
        
        if best_method[0] == 'simple_average':
            final_preds = test_df.mean(axis=1)
        elif best_method[0] == 'weighted_average':
            final_preds = np.average(test_df.values, axis=1, weights=weights)
        else:  # meta_model
            final_preds = meta_model.predict_proba(test_df)[:, 1]
        
        return final_preds
    
    def create_submission_file(self, test_predictions, test_df_original):
        """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        print("=== æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ ===")
        
        # äºŒå€¤åˆ†é¡ã«å¤‰æ›
        binary_preds = (test_predictions >= 0.5).astype(int)
        
        # ãƒ©ãƒ™ãƒ«ã‚’æ–‡å­—åˆ—ã«æˆ»ã™
        reverse_mapping = {v: k for k, v in self.config['TARGET_MAPPING'].items()}
        string_preds = [reverse_mapping[pred] for pred in binary_preds]
        
        # æå‡ºDataFrameä½œæˆ
        submission = pd.DataFrame({
            'id': test_df_original['id'],
            self.config['TARGET_COL']: string_preds
        })
        
        # ä¿å­˜
        submission_path = '/Users/osawa/kaggle/playground-series-s5e7/submissions/final_submission.csv'
        submission.to_csv(submission_path, index=False)
        
        print(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {submission_path}")
        print(f"æå‡ºãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {submission.shape}")
        print(f"äºˆæ¸¬åˆ†å¸ƒ:")
        print(submission[self.config['TARGET_COL']].value_counts())
        
        return submission
    
    def run_final_submission(self):
        """æœ€çµ‚æå‡ºãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("=== æœ€çµ‚æå‡ºãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ ===\n")
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
        X_train, y_train, X_test = self.load_and_preprocess_data()
        test_df_original = pd.read_csv(self.config['DATA_PATH'] / 'test.csv')
        
        # 2. é«˜åº¦ç‰¹å¾´é‡ç”Ÿæˆ
        X_train_enhanced, X_test_enhanced = self.create_advanced_features(X_train, X_test)
        
        # 3. æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨­å®š
        models = self.setup_models()
        
        # 4. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»OOFäºˆæ¸¬
        oof_predictions, model_scores = self.train_models_with_oof(
            models, X_train_enhanced, y_train
        )
        
        # 5. æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        best_method, weights, meta_model = self.create_optimized_ensemble(
            oof_predictions, y_train
        )
        
        # 6. ãƒ†ã‚¹ãƒˆäºˆæ¸¬ç”Ÿæˆ
        test_predictions = self.generate_test_predictions(
            models, X_train_enhanced, y_train, X_test_enhanced,
            best_method, weights, meta_model
        )
        
        # 7. æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        submission = self.create_submission_file(test_predictions, test_df_original)
        
        # 8. çµæœã‚µãƒãƒªãƒ¼
        print(f"\n=== æœ€çµ‚çµæœ ===")
        best_single_model = max(model_scores.items(), key=lambda x: x[1]['score'])
        print(f"æœ€è‰¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«: {best_single_model[0]} ({best_single_model[1]['score']:.6f})")
        print(f"æœ€è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: {best_method[0]} ({best_method[1]:.6f})")
        print(f"ç›®æ¨™0.975708ã¨ã®å·®: {0.975708 - best_method[1]:+.6f}")
        
        if best_method[1] >= 0.975708:
            print("ğŸ‰ ç›®æ¨™ã‚¹ã‚³ã‚¢é”æˆï¼")
        else:
            gap = 0.975708 - best_method[1]
            print(f"ç›®æ¨™ã¾ã§{gap:.6f}ã®æ”¹å–„ãŒå¿…è¦")
        
        return {
            'final_score': best_method[1],
            'model_scores': model_scores,
            'submission': submission
        }

if __name__ == "__main__":
    final_model = FinalSubmissionModel()
    results = final_model.run_final_submission()