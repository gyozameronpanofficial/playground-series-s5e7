"""
Phase 4 æ‹¡å¼µçµ±åˆå®Ÿè£… - CV 0.980+ ç›®æ¨™

Phase 3ã®çµ±åˆæ‰‹æ³•ï¼ˆCV 0.976404ï¼‰ã«ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ :
1. å¤–ã‚Œå€¤ç‰¹å¾´é‡ï¼ˆçµ±è¨ˆçš„é–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
2. æˆ¦ç•¥çš„N-gramç‰¹å¾´é‡ï¼ˆå¿ƒç†å­¦çš„çµ„ã¿åˆã‚ã›ï¼‰
3. ä¸¡å‘æ€§ç‰¹å¾´é‡ï¼ˆãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™ï¼‰
4. é«˜åº¦æ¬ æå€¤å‡¦ç†ï¼ˆpersonality-awareï¼‰

æœŸå¾…åŠ¹æœ: CV 0.976404 â†’ 0.978404 (+0.002000)

Author: Osawa
Date: 2025-07-04
Purpose: GMåˆ†æã‚’è¸ã¾ãˆãŸæ‹¡å¼µå®Ÿè£…
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import KNNImputer
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostClassifier
from sklearn.linear_model import LogisticRegression
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

class EnhancedHybridFeatureEngineer:
    """Phase 4 æ‹¡å¼µçµ±åˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.target_encoders = {}
        self.knn_imputer = KNNImputer(n_neighbors=5)
        
    def create_psychological_features(self, df):
        """å¿ƒç†å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ã®ä½œæˆï¼ˆPhase 3ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        
        print("   å¿ƒç†å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        # Big Fiveç†è«–ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        extroversion_features = ['Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']
        introversion_features = ['Time_spent_Alone', 'Stage_fear', 'Drained_after_socializing']
        
        # æ•°å€¤åŒ–é–¢æ•°
        def convert_to_numeric(series):
            if series.dtype == 'object':
                mapping = {'No': 0, 'Sometimes': 1, 'Yes': 2}
                if series.name in ['Friends_circle_size', 'Post_frequency']:
                    mapping = {'Small/Low': 0, 'Medium': 1, 'Large/High': 2}
                return series.map(mapping).fillna(1)
            return series
        
        df_processed = df.copy()
        
        # å…¨ç‰¹å¾´é‡ã‚’æ•°å€¤åŒ–
        for col in df_processed.columns:
            if col not in ['id', 'Personality']:
                df_processed[col] = convert_to_numeric(df_processed[col])
        
        # å¤–å‘æ€§ã‚¹ã‚³ã‚¢
        extroversion_cols = [col for col in extroversion_features if col in df_processed.columns]
        df_processed['extroversion_score'] = df_processed[extroversion_cols].mean(axis=1)
        
        # å†…å‘æ€§ã‚¹ã‚³ã‚¢
        introversion_cols = [col for col in introversion_features if col in df_processed.columns]
        df_processed['introversion_score'] = df_processed[introversion_cols].mean(axis=1)
        
        # ç¤¾äº¤ãƒãƒ©ãƒ³ã‚¹
        df_processed['social_balance'] = df_processed['extroversion_score'] - df_processed['introversion_score']
        
        # ç¤¾äº¤ç–²åŠ´åº¦
        if 'Drained_after_socializing' in df_processed.columns and 'Social_event_attendance' in df_processed.columns:
            df_processed['social_fatigue'] = df_processed['Drained_after_socializing'] * df_processed['Social_event_attendance']
        
        # ç¤¾äº¤ç©æ¥µåº¦
        if 'Going_outside' in df_processed.columns and 'Friends_circle_size' in df_processed.columns:
            df_processed['social_proactivity'] = df_processed['Going_outside'] * df_processed['Friends_circle_size']
        
        # å­¤ç‹¬å—œå¥½åº¦
        if 'Time_spent_Alone' in df_processed.columns and 'Stage_fear' in df_processed.columns:
            df_processed['solitude_preference'] = df_processed['Time_spent_Alone'] * (2 - df_processed['Stage_fear'])
        
        print(f"     å¿ƒç†å­¦ç‰¹å¾´é‡: 6å€‹")
        return df_processed
    
    def create_outlier_features(self, df):
        """å¤–ã‚Œå€¤ç‰¹å¾´é‡ã®ä½œæˆï¼ˆGMåˆ†æã‚ˆã‚Šï¼‰"""
        
        print("   å¤–ã‚Œå€¤ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df_processed = df.copy()
        
        # 1. Time_spent_Alone ã®é«˜å€¤ãƒ•ãƒ©ã‚°ï¼ˆ94% Introvertç²¾åº¦ï¼‰
        if 'Time_spent_Alone' in df_processed.columns:
            alone_mean = df_processed['Time_spent_Alone'].mean()
            alone_std = df_processed['Time_spent_Alone'].std()
            alone_threshold = alone_mean + 2 * alone_std
            df_processed['extreme_alone_flag'] = (df_processed['Time_spent_Alone'] > alone_threshold).astype(int)
        
        # 2. Stage_fear ã®æ¬ æå€¤ãƒ•ãƒ©ã‚°ï¼ˆ10.22%æ¬ æç‡ï¼‰
        if 'Stage_fear' in df.columns:
            df_processed['stage_fear_missing'] = df['Stage_fear'].isna().astype(int)
        
        # 3. æ¥µç«¯ãªå†…å‘çš„è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
        if all(col in df_processed.columns for col in ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside']):
            alone_high = df_processed['Time_spent_Alone'] > df_processed['Time_spent_Alone'].quantile(0.95)
            social_low = df_processed['Social_event_attendance'] <= 1
            outside_low = df_processed['Going_outside'] <= 1
            df_processed['extreme_introvert_pattern'] = (alone_high & social_low & outside_low).astype(int)
        
        # 4. æ¥µç«¯ãªå¤–å‘çš„è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³
        if all(col in df_processed.columns for col in ['Social_event_attendance', 'Friends_circle_size', 'Post_frequency']):
            social_high = df_processed['Social_event_attendance'] >= 2
            friends_high = df_processed['Friends_circle_size'] >= 2
            post_high = df_processed['Post_frequency'] >= 2
            df_processed['extreme_extrovert_pattern'] = (social_high & friends_high & post_high).astype(int)
        
        # 5. ä¸¡æ¥µç«¯ãƒ•ãƒ©ã‚°
        if all(col in df_processed.columns for col in ['extroversion_score', 'introversion_score']):
            ext_extreme = df_processed['extroversion_score'] > df_processed['extroversion_score'].quantile(0.9)
            int_extreme = df_processed['introversion_score'] > df_processed['introversion_score'].quantile(0.9)
            df_processed['personality_extreme_flag'] = (ext_extreme | int_extreme).astype(int)
        
        print(f"     å¤–ã‚Œå€¤ç‰¹å¾´é‡: 5å€‹")
        return df_processed
    
    def create_strategic_ngrams(self, df):
        """æˆ¦ç•¥çš„N-gramç‰¹å¾´é‡ã®ä½œæˆï¼ˆå¿ƒç†å­¦çš„çµ„ã¿åˆã‚ã›ï¼‰"""
        
        print("   æˆ¦ç•¥çš„N-gramç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df_processed = df.copy()
        
        # å¿ƒç†å­¦çš„ã«æ„å‘³ã®ã‚ã‚‹2-gramçµ„ã¿åˆã‚ã›
        social_combos = [
            ('Social_event_attendance', 'Friends_circle_size'),
            ('Going_outside', 'Post_frequency'),
            ('Social_event_attendance', 'Going_outside'),
            ('Friends_circle_size', 'Post_frequency')
        ]
        
        introvert_combos = [
            ('Time_spent_Alone', 'Stage_fear'),
            ('Time_spent_Alone', 'Drained_after_socializing'),
            ('Stage_fear', 'Drained_after_socializing'),
            ('Time_spent_Alone', 'Social_event_attendance')  # å¯¾ç…§çš„çµ„ã¿åˆã‚ã›
        ]
        
        # 2-gramç‰¹å¾´é‡ä½œæˆ
        for col1, col2 in social_combos + introvert_combos:
            if col1 in df_processed.columns and col2 in df_processed.columns:
                df_processed[f"{col1}_{col2}_combo"] = (
                    df_processed[col1].astype(str) + "_" + df_processed[col2].astype(str)
                )
        
        print(f"     æˆ¦ç•¥çš„N-gramç‰¹å¾´é‡: 8å€‹")
        return df_processed
    
    def create_ambivert_features(self, df):
        """ä¸¡å‘æ€§ç‰¹å¾´é‡ã®ä½œæˆï¼ˆãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™ï¼‰"""
        
        print("   ä¸¡å‘æ€§ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df_processed = df.copy()
        
        # ç¤¾ä¼šæ€§ãƒ»å†…å‘æ€§ã‚¹ã‚³ã‚¢ãŒå¿…è¦
        if 'extroversion_score' in df_processed.columns and 'introversion_score' in df_processed.columns:
            
            # 1. ä¸¡å‘æ€§ã‚¹ã‚³ã‚¢ï¼ˆãƒãƒ©ãƒ³ã‚¹åº¦ï¼‰
            score_diff = abs(df_processed['extroversion_score'] - df_processed['introversion_score'])
            df_processed['ambivert_score'] = 1 / (1 + score_diff)
            
            # 2. æ¥µç«¯åº¦ã‚¹ã‚³ã‚¢
            df_processed['extreme_score'] = score_diff
            
            # 3. ä¸¡å‘æ€§ãƒ•ãƒ©ã‚°ï¼ˆä¸­ç¨‹åº¦ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
            balance_threshold = 0.5
            df_processed['ambivert_flag'] = (df_processed['ambivert_score'] > balance_threshold).astype(int)
            
        print(f"     ä¸¡å‘æ€§ç‰¹å¾´é‡: 3å€‹")
        return df_processed
    
    def advanced_missing_value_handling(self, train_df, test_df):
        """é«˜åº¦æ¬ æå€¤å‡¦ç†ï¼ˆpersonality-aware + KNNï¼‰"""
        
        print("   é«˜åº¦æ¬ æå€¤å‡¦ç†ä¸­...")
        
        train_processed = train_df.copy()
        test_processed = test_df.copy()
        
        # 1. æ•°å€¤ç‰¹å¾´é‡ã®KNNè£œå®Œ
        numeric_cols = []
        for col in train_processed.columns:
            if col not in ['id', 'Personality', 'is_pseudo', 'confidence'] and pd.api.types.is_numeric_dtype(train_processed[col]):
                numeric_cols.append(col)
        
        if numeric_cols:
            # KNNè£œå®Œ
            train_numeric = train_processed[numeric_cols].copy()
            test_numeric = test_processed[numeric_cols].copy()
            
            # æ¬ æå€¤ãŒã‚ã‚‹å ´åˆã®ã¿KNNé©ç”¨
            if train_numeric.isnull().sum().sum() > 0:
                knn_imputer = KNNImputer(n_neighbors=5)
                train_processed[numeric_cols] = knn_imputer.fit_transform(train_numeric)
                test_processed[numeric_cols] = knn_imputer.transform(test_numeric)
        
        # 2. ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ã®æ€§æ ¼ãƒ™ãƒ¼ã‚¹è£œå®Œ
        if 'Personality' in train_processed.columns:
            categorical_cols = []
            for col in train_processed.columns:
                if col not in ['id', 'Personality', 'is_pseudo', 'confidence'] and train_processed[col].dtype == 'object':
                    categorical_cols.append(col)
            
            for col in categorical_cols:
                for personality in ['Extrovert', 'Introvert']:
                    # æ€§æ ¼åˆ¥ã®æœ€é »å€¤ã§è£œå®Œ
                    personality_data = train_processed[train_processed['Personality'] == personality]
                    if len(personality_data) > 0:
                        mode_val = personality_data[col].mode()
                        if len(mode_val) > 0:
                            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®è£œå®Œ
                            mask = (train_processed['Personality'] == personality) & (train_processed[col].isna())
                            train_processed.loc[mask, col] = mode_val.iloc[0]
                            
                            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è£œå®Œï¼ˆå…¨ä½“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼‰
                            test_mask = test_processed[col].isna()
                            test_processed.loc[test_mask, col] = mode_val.iloc[0]
        
        # 3. æ®‹ã‚Šã®æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
        train_processed = train_processed.fillna(0)
        test_processed = test_processed.fillna(0)
        
        print(f"     é«˜åº¦æ¬ æå€¤å‡¦ç†å®Œäº†")
        return train_processed, test_processed
    
    def apply_target_encoding(self, train_df, test_df, target_col='Personality'):
        """Target Encodingã®é©ç”¨ï¼ˆPhase 3ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        
        print("   Target Encodingé©ç”¨ä¸­...")
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®ç‰¹å®š
        categorical_features = []
        for col in train_df.columns:
            if col not in ['id', 'Personality', 'is_pseudo', 'confidence'] and train_df[col].dtype == 'object':
                categorical_features.append(col)
        
        if not categorical_features:
            print("     ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ãªã—ã€Target Encodingã‚¹ã‚­ãƒƒãƒ—")
            return train_df.copy(), test_df.copy()
        
        print(f"     å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {categorical_features}")
        
        # æ•°å€¤åŒ–
        y_train = train_df[target_col].map({'Extrovert': 1, 'Introvert': 0})
        
        train_encoded = train_df.copy()
        test_encoded = test_df.copy()
        
        # CVå†…ã§ã®Target Encoding
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state)
        
        for feature in categorical_features:
            print(f"       {feature}ã®Target Encoding...")
            
            # CVå†…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            encoded_train = np.zeros(len(train_df))
            
            for train_idx, valid_idx in cv.split(train_df, y_train):
                # Train foldã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¾æ›¸ä½œæˆ
                train_fold_feature = train_df.iloc[train_idx][feature].reset_index(drop=True)
                train_fold_target = y_train.iloc[train_idx].reset_index(drop=True)
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡è¨ˆç®—
                encoding_dict = {}
                for cat_val in train_fold_feature.unique():
                    mask = (train_fold_feature == cat_val)
                    if mask.sum() > 0:
                        encoding_dict[cat_val] = train_fold_target[mask].mean()
                
                global_mean = train_fold_target.mean()
                
                # Valid foldã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰é©ç”¨
                valid_feature = train_df.iloc[valid_idx][feature]
                encoded_valid = valid_feature.map(encoding_dict).fillna(global_mean)
                encoded_train[valid_idx] = encoded_valid
            
            # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
            train_encoded[f'{feature}_target_encoded'] = encoded_train
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨ã®å…¨ä½“ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰è¾æ›¸ä½œæˆ
            full_encoding_dict = train_df[feature].groupby(train_df[feature]).apply(
                lambda x: y_train.iloc[x.index].mean()
            ).to_dict()
            
            test_encoded[f'{feature}_target_encoded'] = test_df[feature].map(full_encoding_dict).fillna(y_train.mean())
        
        print(f"     Target Encodingç‰¹å¾´é‡: {len(categorical_features)}å€‹")
        return train_encoded, test_encoded
    
    def create_statistical_features(self, df):
        """çµ±è¨ˆçš„ç‰¹å¾´é‡ã®ä½œæˆï¼ˆPhase 3ã‹ã‚‰ç¶™æ‰¿ï¼‰"""
        
        print("   çµ±è¨ˆçš„ç‰¹å¾´é‡ä½œæˆä¸­...")
        
        df_processed = df.copy()
        
        # æ•°å€¤ç‰¹å¾´é‡ã®çµ±è¨ˆ
        numeric_cols = []
        for col in df_processed.columns:
            if col not in ['id', 'Personality', 'is_pseudo', 'confidence'] and pd.api.types.is_numeric_dtype(df_processed[col]):
                numeric_cols.append(col)
        
        if len(numeric_cols) > 1:
            numeric_data = df_processed[numeric_cols]
            
            # ç‰¹å¾´é‡çµ±è¨ˆ
            df_processed['feature_mean'] = numeric_data.mean(axis=1)
            df_processed['feature_std'] = numeric_data.std(axis=1)
            df_processed['feature_max'] = numeric_data.max(axis=1)
            df_processed['feature_min'] = numeric_data.min(axis=1)
            
            print(f"     çµ±è¨ˆçš„ç‰¹å¾´é‡: 4å€‹")
        else:
            print("     æ•°å€¤ç‰¹å¾´é‡ä¸è¶³ã€çµ±è¨ˆçš„ç‰¹å¾´é‡ã‚¹ã‚­ãƒƒãƒ—")
        
        return df_processed
    
    def create_enhanced_features(self, train_df, test_df):
        """æ‹¡å¼µçµ±åˆç‰¹å¾´é‡ä½œæˆãƒ¡ã‚¤ãƒ³é–¢æ•°"""
        
        print("=== Phase 4 æ‹¡å¼µçµ±åˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ ===")
        print(f"å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ - è¨“ç·´: {train_df.shape}, ãƒ†ã‚¹ãƒˆ: {test_df.shape}")
        
        # 1. é«˜åº¦æ¬ æå€¤å‡¦ç†ï¼ˆæœ€åˆã«å®Ÿè¡Œï¼‰
        train_processed, test_processed = self.advanced_missing_value_handling(train_df, test_df)
        
        # 2. å¿ƒç†å­¦ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ï¼ˆPhase 3ã‹ã‚‰ï¼‰
        train_psych = self.create_psychological_features(train_processed)
        test_psych = self.create_psychological_features(test_processed)
        
        # 3. å¤–ã‚Œå€¤ç‰¹å¾´é‡ï¼ˆæ–°è¦ï¼‰
        train_outlier = self.create_outlier_features(train_psych)
        test_outlier = self.create_outlier_features(test_psych)
        
        # 4. æˆ¦ç•¥çš„N-gramç‰¹å¾´é‡ï¼ˆæ–°è¦ï¼‰
        train_ngram = self.create_strategic_ngrams(train_outlier)
        test_ngram = self.create_strategic_ngrams(test_outlier)
        
        # 5. ä¸¡å‘æ€§ç‰¹å¾´é‡ï¼ˆæ–°è¦ï¼‰
        train_ambi = self.create_ambivert_features(train_ngram)
        test_ambi = self.create_ambivert_features(test_ngram)
        
        # 6. Target Encodingï¼ˆPhase 3ã‹ã‚‰ï¼‰
        train_encoded, test_encoded = self.apply_target_encoding(train_ambi, test_ambi)
        
        # 7. çµ±è¨ˆçš„ç‰¹å¾´é‡ï¼ˆPhase 3ã‹ã‚‰ï¼‰
        train_final = self.create_statistical_features(train_encoded)
        test_final = self.create_statistical_features(test_encoded)
        
        print(f"æœ€çµ‚ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ - è¨“ç·´: {train_final.shape}, ãƒ†ã‚¹ãƒˆ: {test_final.shape}")
        print(f"ç‰¹å¾´é‡å¢—åŠ æ•°: {train_final.shape[1] - train_df.shape[1]}å€‹")
        
        # ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
        print(f"\\nğŸ”§ ç‰¹å¾´é‡ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼:")
        print(f"   Phase 3ç¶™æ‰¿: å¿ƒç†å­¦(6) + Target Encoding + çµ±è¨ˆ(4) = ç´„17å€‹")
        print(f"   Phase 4æ–°è¦: å¤–ã‚Œå€¤(5) + N-gram(8) + ä¸¡å‘æ€§(3) = 16å€‹")
        print(f"   åˆè¨ˆäºˆæƒ³: ç´„33å€‹ç‰¹å¾´é‡")
        
        return train_final, test_final

def create_pseudo_labeled_data(train_features, test_features, confidence_threshold=0.85):
    """æ“¬ä¼¼ãƒ©ãƒ™ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆï¼ˆå¼•æ•°å¤‰æ›´ï¼‰"""
    
    print("=== æ“¬ä¼¼ãƒ©ãƒ™ãƒªãƒ³ã‚°å®Ÿè¡Œ ===")
    
    # å‰å‡¦ç†
    feature_cols = [col for col in train_features.columns if col not in ['id', 'Personality', 'is_pseudo', 'confidence']]
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    X_train = train_features[feature_cols].copy()
    X_test = test_features[feature_cols].copy()
    
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col], X_test[col]]).astype(str)
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
    
    X_train = X_train.fillna(0).values
    X_test = X_test.fillna(0).values
    y_train = train_features['Personality'].map({'Extrovert': 1, 'Introvert': 0}).values
    
    # æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«
    pseudo_models = [
        lgb.LGBMClassifier(n_estimators=1000, random_state=42, verbosity=-1),
        xgb.XGBClassifier(n_estimators=1000, random_state=42, verbosity=0),
        CatBoostClassifier(iterations=1000, random_seed=42, verbose=False)
    ]
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    test_predictions = []
    
    for i, model in enumerate(pseudo_models):
        print(f"   ãƒ¢ãƒ‡ãƒ«{i+1}è¨“ç·´ä¸­...")
        model.fit(X_train, y_train)
        pred_proba = model.predict_proba(X_test)[:, 1]
        test_predictions.append(pred_proba)
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å¹³å‡
    ensemble_proba = np.mean(test_predictions, axis=0)
    
    # é«˜ä¿¡é ¼åº¦ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
    confident_mask = (ensemble_proba >= confidence_threshold) | (ensemble_proba <= 1 - confidence_threshold)
    confident_indices = np.where(confident_mask)[0]
    
    if len(confident_indices) == 0:
        print("   é«˜ä¿¡é ¼åº¦ã‚µãƒ³ãƒ—ãƒ«ãªã—ã€å…ƒãƒ‡ãƒ¼ã‚¿ã®ã¿è¿”å´")
        return train_features
    
    # æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ä½œæˆ
    pseudo_labels = (ensemble_proba[confident_indices] >= 0.5).astype(int)
    pseudo_labels_str = ['Extrovert' if label == 1 else 'Introvert' for label in pseudo_labels]
    
    # æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    pseudo_df = test_features.iloc[confident_indices].copy()
    pseudo_df['Personality'] = pseudo_labels_str
    pseudo_df['is_pseudo'] = True
    pseudo_df['confidence'] = np.maximum(ensemble_proba[confident_indices], 
                                       1 - ensemble_proba[confident_indices])
    
    # å…ƒãƒ‡ãƒ¼ã‚¿ã«ãƒ•ãƒ©ã‚°è¿½åŠ 
    train_features['is_pseudo'] = False
    train_features['confidence'] = 1.0
    
    # çµåˆ
    augmented_data = pd.concat([train_features, pseudo_df], ignore_index=True)
    
    print(f"   å…ƒãƒ‡ãƒ¼ã‚¿: {len(train_features)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ“¬ä¼¼ãƒ©ãƒ™ãƒ«: {len(pseudo_df)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ç·è¨ˆ: {len(augmented_data)}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   æ‹¡å¼µç‡: {len(pseudo_df)/len(train_features)*100:.1f}%")
    
    return augmented_data

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("=== Phase 4 æ‹¡å¼µçµ±åˆå®Ÿè£… ===")
    print("Phase 3ã®æˆåŠŸã‚’åŸºç›¤ã¨ã—ã¦ã€GMåˆ†æã«ã‚ˆã‚‹æ”¹å–„ã‚’è¿½åŠ å®Ÿè£…")
    
    # 1. å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\\n1. å…ƒãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/raw/train.csv')
    test_data = pd.read_csv('/Users/osawa/kaggle/playground-series-s5e7/data/raw/test.csv')
    
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_data.shape}")
    print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_data.shape}")
    
    # 2. æ‹¡å¼µçµ±åˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    print("\\n2. æ‹¡å¼µçµ±åˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    feature_engineer = EnhancedHybridFeatureEngineer()
    train_features, test_features = feature_engineer.create_enhanced_features(train_data, test_data)
    
    # 3. æ“¬ä¼¼ãƒ©ãƒ™ãƒªãƒ³ã‚°
    print("\\n3. æ“¬ä¼¼ãƒ©ãƒ™ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    augmented_train = create_pseudo_labeled_data(train_features, test_features)
    
    # 4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    print("\\n4. ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­...")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    train_path = '/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase4_train_features.csv'
    augmented_train.to_csv(train_path, index=False)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
    test_path = '/Users/osawa/kaggle/playground-series-s5e7/data/processed/phase4_test_features.csv'
    test_features.to_csv(test_path, index=False)
    
    print(f"âœ… Phase 4 æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†!")
    print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {augmented_train.shape} â†’ {train_path}")
    print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_features.shape} â†’ {test_path}")
    
    # 5. çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print(f"\\nğŸ“Š Phase 4 å®Ÿè£…ã‚µãƒãƒªãƒ¼:")
    print(f"   Phase 3ç‰¹å¾´é‡æ•°: ç´„17å€‹")
    print(f"   Phase 4ç‰¹å¾´é‡æ•°: {test_features.shape[1] - 1}å€‹")  # idã‚’é™¤ã
    print(f"   è¿½åŠ ç‰¹å¾´é‡æ•°: {test_features.shape[1] - 18}å€‹")  # id + 17å€‹ã‚’é™¤ã
    print(f"   æ“¬ä¼¼ãƒ©ãƒ™ãƒ«æ‹¡å¼µç‡: {(len(augmented_train) - len(train_features))/len(train_features)*100:.1f}%")
    
    # 6. æ”¹å–„è¦ç´ è©³ç´°
    print(f"\\nğŸ”§ Phase 4 æ”¹å–„è¦ç´ :")
    print(f"   1. é«˜åº¦æ¬ æå€¤å‡¦ç†: KNN + personality-awareè£œå®Œ")
    print(f"   2. å¤–ã‚Œå€¤ç‰¹å¾´é‡: çµ±è¨ˆçš„é–¾å€¤ãƒ™ãƒ¼ã‚¹ 5å€‹")
    print(f"   3. æˆ¦ç•¥çš„N-gram: å¿ƒç†å­¦çš„çµ„ã¿åˆã‚ã› 8å€‹")
    print(f"   4. ä¸¡å‘æ€§ç‰¹å¾´é‡: ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™ 3å€‹")
    print(f"   5. Phase 3ç¶™æ‰¿: å¿ƒç†å­¦+Target Encoding+çµ±è¨ˆ+æ“¬ä¼¼ãƒ©ãƒ™ãƒ«")
    
    print(f"\\nğŸ¯ Phase 4 æœŸå¾…åŠ¹æœ:")
    print(f"   Phase 3 CVãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: 0.976404")
    print(f"   Phase 4 CVäºˆæƒ³: 0.978404 (+0.002000)")
    print(f"   Phase 4 PBäºˆæƒ³: 0.977000+ (GMè¶…è¶Š)")
    
    print(f"\\nğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"   1. Phase 4 CVè©•ä¾¡å®Ÿè¡Œ")
    print(f"   2. Phase 3 vs Phase 4 æ€§èƒ½æ¯”è¼ƒ")
    print(f"   3. æœ€é©æ‰‹æ³•ã®æ±ºå®š")
    
    return augmented_train, test_features

if __name__ == "__main__":
    main()